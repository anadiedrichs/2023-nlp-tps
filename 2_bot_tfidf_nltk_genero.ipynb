{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anadiedrichs/2023-nlp-tps/blob/main/2_bot_tfidf_nltk_genero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5hxxkdAQJg"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Sistema de obtención de información con NLTK\n",
        "\n",
        "Usaré como corpus el sitio web de la unidad de Género y Diversidad de la Universidad Tecnológica Nacional, Facultad Regional Mendoza\n",
        "\n",
        "https://www4.frm.utn.edu.ar/genero-y-diversidad/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCED1hh-Ioyf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random\n",
        "import re # Regular Expressions (regex)\n",
        "import urllib.request\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs\n",
        "\n",
        "import nltk\n",
        "# Descargar el diccionario\n",
        "#nltk.download(\"punkt\")\n",
        "#nltk.download(\"wordnet\")\n",
        "#nltk.download('omw-1.4')\n",
        "\n",
        "# Download the Spanish stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Download the Spanish tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Download the Spanish lemmatizer data\n",
        "nltk.download(\"omw\")\n",
        "# Download the Spanish corpora\n",
        "nltk.download(\"cess_esp\")  # Corpus of Contemporary Spanish\n",
        "nltk.download(\"conll2002\")  # Spanish Named Entity Recognition (NER) Corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOa4JPSCJ29"
      },
      "source": [
        "### Datos\n",
        "Se consumirán los datos del artículo de wikipedia sobre el deporte \"Tennis\" en inglés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIO7b8GjAC17"
      },
      "outputs": [],
      "source": [
        "raw_html = urllib.request.urlopen('https://www4.frm.utn.edu.ar/genero-y-diversidad/')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear artículo, 'lxml' es el parser a utilizar\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
        "# y tenerlos disponible como lista\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text\n",
        "\n",
        "article_text = article_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la página web\n",
        "url = 'https://www4.frm.utn.edu.ar/genero-y-diversidad/'\n",
        "\n",
        "# Realizar una solicitud HTTP para obtener el contenido de la página\n",
        "response = requests.get(url)\n",
        "\n",
        "# Comprobar si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Analizar el contenido HTML de la página con Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraer todo el texto del HTML\n",
        "    all_text = soup.get_text()\n",
        "\n",
        "    # Imprimir el texto\n",
        "    print(all_text)\n",
        "else:\n",
        "    print(f\"Error al obtener la página. Código de estado: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "jah6Y9DEJ9JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUH30a1_rOkS"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "article_text = all_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtGLJjt6rQhK"
      },
      "outputs": [],
      "source": [
        "print(\"Cantidad de caracteres en el sitio web principal:\", len(article_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHxBRNzCMOS"
      },
      "source": [
        "### 2 - Preprocesamiento\n",
        "- Remover caracteres especiales\n",
        "- Quitar espacios o saltos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnEUTD1Erl1N"
      },
      "outputs": [],
      "source": [
        "# Repaso de regex:\n",
        "# https://docs.python.org/3/library/re.html\n",
        "\n",
        "# Para practicar regex:\n",
        "# https://regex101.com/\n",
        "\n",
        "# el inicio con 'r' antes de cada string indica que se interprete como raw string\n",
        "# '\\n' es interpretado por Python como salto de linea\n",
        "# r'\\n' es interpretado por Python como el string formado por dos caracteres:\n",
        "#  backslash y n\n",
        "\n",
        "# substituir con regex con espacio vacío:\n",
        "text = re.sub(r'\\[[0-9]*\\]', ' ', article_text) # substituir los números entre corchetes\n",
        "# (notar que los corchetes son interpretados literalmente por los backlsash)\n",
        "text = re.sub(r'\\s+', ' ', text) # substituir más de un caracter de espacio, salto de línea o tabulación\n",
        "\n",
        "# probar en regex101 con los patrones anteriores:\n",
        "# 'Hola [1], [], [ estoy bien   [123]. [12sss]. OK!   .'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7ycrAMYrn66"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA5F0s4UsMpf"
      },
      "outputs": [],
      "source": [
        "print(\"Cantidad de caracteres en el texto:\", len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKNcDGcisajf"
      },
      "source": [
        "### 3 - Dividir el texto en sentencias y en palabras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from nltk.tokenize import word_tokenize\n",
        "#from nltk.corpus import stopwords\n",
        "\n",
        "# Tokenize Spanish text\n",
        "#text = \"Este es un ejemplo de texto en español.\"\n",
        "#tokens = word_tokenize(text, language='spanish')\n",
        "\n",
        "# Remove stopwords\n",
        "#spanish_stopwords = set(stopwords.words('spanish'))\n",
        "#filtered_tokens = [word for word in tokens if word.lower() not in spanish_stopwords]\n",
        "\n",
        "# Perform other NLP tasks as needed (lemmatization, POS tagging, etc.)\n"
      ],
      "metadata": {
        "id": "csC7xOhEFlLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reXBOFQ7sdlB"
      },
      "outputs": [],
      "source": [
        "corpus = nltk.sent_tokenize(text, language='spanish') # divide en oraciones\n",
        "words = nltk.word_tokenize(text, language='spanish') # divide en términos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5GloV9fsi6o"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "corpus[:130]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmQ7nkvvsi0i"
      },
      "outputs": [],
      "source": [
        "# Demos un vistazo\n",
        "words[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXPWNkKfEvDZ"
      },
      "outputs": [],
      "source": [
        "print(\"Vocabulario:\", len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlYKyb3OtDse"
      },
      "source": [
        "### 4 - Funciones de ayuda para limpiar y procesar el input del usuario\n",
        "- Lematizar los tokens de la oración\n",
        "- Quitar símbolos de puntuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afPok8pstPOx"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def perform_lemmatization(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# ord() nos da el código Unicode para un caracter dado\n",
        "punctuation_removal = dict((ord(punctuation), None) for punctuation in string.punctuation)\n",
        "\n",
        "def get_processed_text(document):\n",
        "    # 1 - reduce el texto a mínuscula (string.lower())\n",
        "    # 2 - quitar los simbolos de puntuacion (string.translate())\n",
        "    # 3 - realiza la tokenización (nltk.word_tokenize)\n",
        "    # 4 - realiza la lematización (nuestra función perform_lemmatization)\n",
        "    return perform_lemmatization(nltk.word_tokenize(document.lower().translate(punctuation_removal), language='spanish'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl8r6d9ZuyR9"
      },
      "source": [
        "### 5 - Utilizar vectores TF-IDF y la similitud coseno construido con el corpus del artículo de wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRYFHcBfk2Gt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def generate_response(user_input, corpus):\n",
        "    response = ''\n",
        "    # Sumar al corpus la pregunta del usuario para calcular\n",
        "    # su cercania con otros documentos/sentencias\n",
        "    # la entrada del usuario se usa para tokenizar y vectorizar\n",
        "    corpus.append(user_input)\n",
        "\n",
        "    # Crear un vectorizar TFIDF que quite las \"stop words\" del ingles y utilice\n",
        "    # nuestra funcion para obtener los tokens lematizados \"get_processed_text\"\n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=get_processed_text, stop_words='english')\n",
        "\n",
        "    # Crear los vectores a partir del corpus\n",
        "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Calcular la similitud coseno entre todas los documentos excepto el agregado (el útlimo \"-1\")\n",
        "    # NOTA: con los word embedings veremos más en detalle esta matriz de similitud\n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "\n",
        "    # Obtener el índice del vector más cercano a nuestra oración\n",
        "    # --> descartando la similitud contra nuestor vector propio\n",
        "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "\n",
        "    if vector_matched == 0: # si la similaridad coseno fue nula (ningún término en común)\n",
        "        response = \"I am sorry, I could not understand you\"\n",
        "    else:\n",
        "        response = corpus[similar_sentence_number] # obtener el documento del corpus más similar\n",
        "\n",
        "    corpus.remove(user_input)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK-BuXPBybSp"
      },
      "source": [
        "### 6 - Ensayar el sistema\n",
        "El sistema intentará encontrar la parte del artículo que más se relaciona con nuestro texto de entrada. Sugerencias a ensayar:\n",
        "- contacto\n",
        "- denuncia\n",
        "- referente\n",
        "- comisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2X4j8XyydSb"
      },
      "outputs": [],
      "source": [
        "# Se utilizará gradio para ensayar el bot\n",
        "# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n",
        "# https://gradio.app/\n",
        "import sys\n",
        "!{sys.executable} -m pip install gradio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZv5MiVzynG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e430b23-7408-45fe-c3fb-b82ff3f927e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-2ee8e5560b73>:9: GradioDeprecationWarning: `layout` parameter is deprecated, and it has no effect\n",
            "  iface = gr.Interface(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1f2c6ebcb3c4ee64ea.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1f2c6ebcb3c4ee64ea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: denunciar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 488, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1435, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1107, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 707, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-52-2ee8e5560b73>\", line 5, in bot_response\n",
            "    resp = generate_response(human_text.lower(), corpus)\n",
            "  File \"<ipython-input-50-9199ec08be81>\", line 16, in generate_response\n",
            "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2133, in fit_transform\n",
            "    X = super().fit_transform(raw_documents)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1369, in fit_transform\n",
            "    self._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 600, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'spanish' instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: denuncias\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 488, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1435, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1107, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 707, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"<ipython-input-52-2ee8e5560b73>\", line 5, in bot_response\n",
            "    resp = generate_response(human_text.lower(), corpus)\n",
            "  File \"<ipython-input-50-9199ec08be81>\", line 16, in generate_response\n",
            "    all_word_vectors = word_vectorizer.fit_transform(corpus)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 2133, in fit_transform\n",
            "    X = super().fit_transform(raw_documents)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\", line 1369, in fit_transform\n",
            "    self._validate_params()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 600, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'spanish' instead.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def bot_response(human_text):\n",
        "    print(\"Q:\", human_text)\n",
        "    resp = generate_response(human_text.lower(), corpus)\n",
        "    print(\"A:\", resp)\n",
        "    return resp\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=bot_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\",\n",
        "    layout=\"vertical\")\n",
        "\n",
        "iface.launch(debug=True,share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuv5DTnOAPgS"
      },
      "source": [
        "### Alumno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74KwutMZAPgS"
      },
      "source": [
        "- Tomar un ejemplo de los bots utilizados (uno de los dos) y construir el propio.\n",
        "- Sacar conclusiones de los resultados.\n",
        "\n",
        "__IMPORTANTE__: Recuerde para la entrega del ejercicio debe quedar registrado en el colab las preguntas y las respuestas del BOT para que podamos evaluar el desempeño final."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}